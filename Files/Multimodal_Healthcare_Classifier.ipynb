{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ccc904",
   "metadata": {},
   "source": [
    "\n",
    "# Multimodal Healthcare Classifier (CV + NLP) — PneumoniaMNIST\n",
    "\n",
    "**Highlights:** Deep Learning • Computer Vision • NLP • Multimodal Fusion • PyTorch • Hugging Face\n",
    "\n",
    "This notebook trains three models on a lightweight medical dataset:\n",
    "- **Image-only**: ResNet-18 on chest X-rays (PneumoniaMNIST, via MedMNIST)\n",
    "- **Text-only**: DistilBERT on *synthetic* short clinical notes correlated with the labels\n",
    "- **Fusion**: Combines image and text embeddings for improved performance\n",
    "\n",
    "**Why it's portfolio-ready**  \n",
    "Clean config, baselines vs fusion, clear metrics (Accuracy/F1), confusion matrices, checkpoints, and reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a7962",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Environment setup (run locally)\n",
    "Uncomment and run the cell below **in your local environment**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%capture\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install transformers medmnist scikit-learn matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff9360",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Imports & Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c482d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from medmnist import PneumoniaMNIST\n",
    "from medmnist import INFO as MED_INFO\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 42\n",
    "    img_size: int = 224\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 2\n",
    "    lr: float = 2e-4\n",
    "    epochs_fusion: int = 5\n",
    "    epochs_baseline: int = 3\n",
    "    model_dir: str = \"checkpoints\"\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    text_model_name: str = \"distilbert-base-uncased\"\n",
    "    max_text_len: int = 64\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.model_dir, exist_ok=True)\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "print(\"Device:\", cfg.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666a6be",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Utilities (metrics & plotting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ed34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion(y_true, y_pred, labels, title, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
    "    im = ax.imshow(cm, interpolation='nearest')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label',\n",
    "           title=title)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def compute_metrics(y_true, y_pred, name: str):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "    print(f\"[{name}] Acc: {acc:.4f} | F1: {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"normal\",\"pneumonia\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127c662",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Data: PneumoniaMNIST (MedMNIST) & transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b616046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_medmnist_splits() -> Tuple[PneumoniaMNIST, PneumoniaMNIST, PneumoniaMNIST]:\n",
    "    info = MED_INFO[\"pneumoniamnist\"]\n",
    "    print(f\"PneumoniaMNIST: {info['description']} | Task: {info['task']} | Labels: {info['label']}\")\n",
    "    train_set = PneumoniaMNIST(split='train', download=True)\n",
    "    val_set   = PneumoniaMNIST(split='val', download=True)\n",
    "    test_set  = PneumoniaMNIST(split='test', download=True)\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "def make_transforms(img_size: int):\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: t.repeat(3,1,1)),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: t.repeat(3,1,1)),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    return train_tf, eval_tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9993ec2",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Synthetic clinical notes (NLP) + Dataset wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def synthesize_note(label: int) -> str:\n",
    "    normal_templates = [\n",
    "        \"Patient with mild cough, afebrile, clear breath sounds, O2 sat stable.\",\n",
    "        \"No respiratory distress noted; chest auscultation unremarkable.\",\n",
    "        \"Vitals within normal limits; denies dyspnea; lungs clear to auscultation.\",\n",
    "    ]\n",
    "    pna_templates = [\n",
    "        \"Productive cough with fever; crackles at bases; increased WBC; suspected pneumonia.\",\n",
    "        \"Shortness of breath and chills; focal consolidation on exam; starting antibiotics.\",\n",
    "        \"Febrile with hypoxia; chest findings consistent with community-acquired pneumonia.\",\n",
    "    ]\n",
    "    additives = [\n",
    "        \"Hx HTN.\", \"No allergies.\", \"Smoker; counseling provided.\", \"Recent viral illness.\",\n",
    "        \"Follow-up CXR recommended.\", \"Sputum culture pending.\"\n",
    "    ]\n",
    "    base = random.choice(pna_templates if label == 1 else normal_templates)\n",
    "    extra = \" \".join(random.sample(additives, k=random.randint(0,2)))\n",
    "    return (base + \" \" + extra).strip()\n",
    "\n",
    "class MultimodalPneumo(Dataset):\n",
    "    def __init__(self, base: PneumoniaMNIST, tf, tokenizer, max_len: int, seed: int = 42):\n",
    "        self.base = base\n",
    "        self.tf = tf\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        random.seed(seed)\n",
    "        self.notes = [synthesize_note(int(label)) for _, label in self.base]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.base[idx]\n",
    "        label = int(label)\n",
    "        img = np.array(img.squeeze(), dtype=np.uint8)\n",
    "        img = self.tf(img)\n",
    "\n",
    "        text = self.notes[idx]\n",
    "        enc = self.tokenizer(text, truncation=True, max_length=self.max_len,\n",
    "                             padding=\"max_length\", return_tensors=\"pt\")\n",
    "        enc = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        return {\n",
    "            \"pixel_values\": img,\n",
    "            \"input_ids\": enc[\"input_ids\"],\n",
    "            \"attention_mask\": enc[\"attention_mask\"],\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"text\": text\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2685a61b",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Models: Image encoder, Text encoder, and Fusion heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce384db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, out_dim=128, pretrained=True):\n",
    "        super().__init__()\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        in_feats = m.fc.in_features\n",
    "        m.fc = nn.Identity()\n",
    "        self.backbone = m\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_feats, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        z = self.proj(h)\n",
    "        return z\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name: str, out_dim=128, freeze_bert=True):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        if freeze_bert:\n",
    "            for p in self.bert.parameters():\n",
    "                p.requires_grad = False\n",
    "        hidden = self.bert.config.hidden_size\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, out_dim)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = outputs.last_hidden_state[:,0]\n",
    "        z = self.proj(cls)\n",
    "        return z\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, img_dim=128, txt_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(img_dim + txt_dim, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, zi, zt):\n",
    "        x = torch.cat([zi, zt], dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class ImageOnlyClassifier(nn.Module):\n",
    "    def __init__(self, img_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(img_dim, 128), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, zi):\n",
    "        return self.head(zi)\n",
    "\n",
    "class TextOnlyClassifier(nn.Module):\n",
    "    def __init__(self, txt_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(txt_dim, 128), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, zt):\n",
    "        return self.head(zt)\n",
    "\n",
    "class MultimodalSystem(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.img_enc = ImageEncoder(out_dim=128, pretrained=True)\n",
    "        self.txt_enc = TextEncoder(cfg.text_model_name, out_dim=128, freeze_bert=True)\n",
    "        self.fusion = FusionClassifier(img_dim=128, txt_dim=128, num_classes=2)\n",
    "    def forward(self, imgs, input_ids, attention_mask):\n",
    "        zi = self.img_enc(imgs)\n",
    "        zt = self.txt_enc(input_ids, attention_mask)\n",
    "        logits = self.fusion(zi, zt)\n",
    "        return logits\n",
    "\n",
    "class ImageOnlySystem(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.img_enc = ImageEncoder(out_dim=128, pretrained=True)\n",
    "        self.cls = ImageOnlyClassifier(img_dim=128, num_classes=2)\n",
    "    def forward(self, imgs, *_):\n",
    "        zi = self.img_enc(imgs)\n",
    "        return self.cls(zi)\n",
    "\n",
    "class TextOnlySystem(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.txt_enc = TextEncoder(cfg.text_model_name, out_dim=128, freeze_bert=True)\n",
    "        self.cls = TextOnlyClassifier(txt_dim=128, num_classes=2)\n",
    "    def forward(self, _, input_ids, attention_mask):\n",
    "        zt = self.txt_enc(input_ids, attention_mask)\n",
    "        return self.cls(zt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fdccf4",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Train / Eval loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779caaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_epoch(model, loaders, optim, criterion, stage, device):\n",
    "    is_train = stage == \"train\"\n",
    "    model.train(is_train)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    pbar = tqdm(loaders, desc=f\"{stage}\", leave=False)\n",
    "    for batch in pbar:\n",
    "        imgs = batch[\"pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        if is_train:\n",
    "            optim.zero_grad()\n",
    "\n",
    "        logits = model(imgs, input_ids, attn)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(dim=1).detach().cpu().numpy().tolist()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(labels.detach().cpu().numpy().tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(loaders.dataset)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return avg_loss, acc, f1, (y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430e9b6",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Train baselines and fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name)\n",
    "train_raw, val_raw, test_raw = get_medmnist_splits()\n",
    "tf_train, tf_eval = make_transforms(cfg.img_size)\n",
    "\n",
    "train_ds = MultimodalPneumo(train_raw, tf_train, tokenizer, cfg.max_text_len, seed=cfg.seed)\n",
    "val_ds   = MultimodalPneumo(val_raw,   tf_eval,  tokenizer, cfg.max_text_len, seed=cfg.seed)\n",
    "test_ds  = MultimodalPneumo(test_raw,  tf_eval,  tokenizer, cfg.max_text_len, seed=cfg.seed)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Image-only\n",
    "print(\"\\n=== Baseline: Image-Only ===\")\n",
    "img_only = ImageOnlySystem().to(cfg.device)\n",
    "optim_i = torch.optim.AdamW([p for p in img_only.parameters() if p.requires_grad], lr=cfg.lr)\n",
    "best_val = 0.0\n",
    "for epoch in range(cfg.epochs_baseline):\n",
    "    tr = run_epoch(img_only, train_loader, optim_i, criterion, \"train\", cfg.device)\n",
    "    vl = run_epoch(img_only, val_loader,   optim_i, criterion, \"eval\",  cfg.device)\n",
    "    print(f\"[ImageOnly][Epoch {epoch+1}] train_loss={tr[0]:.4f} val_acc={vl[1]:.4f} val_f1={vl[2]:.4f}\")\n",
    "    if vl[1] > best_val:\n",
    "        best_val = vl[1]\n",
    "        torch.save(img_only.state_dict(), os.path.join(cfg.model_dir, \"image_only.pt\"))\n",
    "\n",
    "# --- Text-only\n",
    "print(\"\\n=== Baseline: Text-Only ===\")\n",
    "txt_only = TextOnlySystem(cfg).to(cfg.device)\n",
    "optim_t = torch.optim.AdamW([p for p in txt_only.parameters() if p.requires_grad], lr=cfg.lr)\n",
    "best_val = 0.0\n",
    "for epoch in range(cfg.epochs_baseline):\n",
    "    tr = run_epoch(txt_only, train_loader, optim_t, criterion, \"train\", cfg.device)\n",
    "    vl = run_epoch(txt_only, val_loader,   optim_t, criterion, \"eval\",  cfg.device)\n",
    "    print(f\"[TextOnly][Epoch {epoch+1}] train_loss={tr[0]:.4f} val_acc={vl[1]:.4f} val_f1={vl[2]:.4f}\")\n",
    "    if vl[1] > best_val:\n",
    "        best_val = vl[1]\n",
    "        torch.save(txt_only.state_dict(), os.path.join(cfg.model_dir, \"text_only.pt\"))\n",
    "\n",
    "# --- Fusion\n",
    "print(\"\\n=== Multimodal Fusion (Image + Text) ===\")\n",
    "mm = MultimodalSystem(cfg).to(cfg.device)\n",
    "optim_f = torch.optim.AdamW([p for p in mm.parameters() if p.requires_grad], lr=cfg.lr)\n",
    "best_val = 0.0\n",
    "for epoch in range(cfg.epochs_fusion):\n",
    "    tr = run_epoch(mm, train_loader, optim_f, criterion, \"train\", cfg.device)\n",
    "    vl = run_epoch(mm, val_loader,   optim_f, criterion, \"eval\",  cfg.device)\n",
    "    print(f\"[Fusion][Epoch {epoch+1}] train_loss={tr[0]:.4f} val_acc={vl[1]:.4f} val_f1={vl[2]:.4f}\")\n",
    "    if vl[1] > best_val:\n",
    "        best_val = vl[1]\n",
    "        torch.save(mm.state_dict(), os.path.join(cfg.model_dir, \"fusion.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07cd9f4",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Evaluate on test set + Confusion matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, loader, name):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            imgs = b[\"pixel_values\"].to(cfg.device)\n",
    "            input_ids = b[\"input_ids\"].to(cfg.device)\n",
    "            attn = b[\"attention_mask\"].to(cfg.device)\n",
    "            labels = b[\"label\"].to(cfg.device)\n",
    "            logits = model(imgs, input_ids, attn)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(labels.cpu().numpy().tolist())\n",
    "    compute_metrics(y_true, y_pred, name)\n",
    "    plot_confusion(y_true, y_pred, [\"normal\",\"pneumonia\"], f\"Confusion ({name})\")\n",
    "\n",
    "# Load best checkpoints (if training cell was run)\n",
    "if os.path.exists(os.path.join(cfg.model_dir, \"image_only.pt\")):\n",
    "    img_only.load_state_dict(torch.load(os.path.join(cfg.model_dir, \"image_only.pt\"), map_location=cfg.device))\n",
    "if os.path.exists(os.path.join(cfg.model_dir, \"text_only.pt\")):\n",
    "    txt_only.load_state_dict(torch.load(os.path.join(cfg.model_dir, \"text_only.pt\"), map_location=cfg.device))\n",
    "if os.path.exists(os.path.join(cfg.model_dir, \"fusion.pt\")):\n",
    "    mm.load_state_dict(torch.load(os.path.join(cfg.model_dir, \"fusion.pt\"), map_location=cfg.device))\n",
    "\n",
    "evaluate_model(img_only, test_loader, \"image_only\")\n",
    "evaluate_model(txt_only, test_loader, \"text_only\")\n",
    "evaluate_model(mm, test_loader, \"fusion\")\n",
    "\n",
    "# Save quick summary\n",
    "results = []\n",
    "# For a concise record, we can re-run evaluations capturing metrics into results.json as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9086681",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Optional extensions\n",
    "- Unfreeze last ResNet block / BERT layers for fine-tuning (lower LR).\n",
    "- Swap DistilBERT with **ClinicalBERT** (if licensing/data access permits) for more clinical language.\n",
    "- Add **Grad-CAM** for image explainability & attention visualization for text.\n",
    "- Log experiments with **Weights & Biases** or **MLflow**.\n",
    "- Replace synthetic notes with a de-identified corpus when available.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
